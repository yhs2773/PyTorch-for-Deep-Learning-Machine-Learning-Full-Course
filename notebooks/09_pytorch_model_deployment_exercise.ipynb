{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyMJ2Yi22N4yU1qDyehpQhO4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# Exercise"],"metadata":{"id":"C5zG6sqdNbxh"}},{"cell_type":"markdown","source":["## 0. Prerequisites"],"metadata":{"id":"DatxYzOLNdwh"}},{"cell_type":"code","source":["# install packages\n","!pip install -qq torchinfo"],"metadata":{"id":"WU6vm82_N8OC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# import scripts from GitHub\n","!git clone https://github.com/yhs2773/PyTorch-for-Deep-Learning-Machine-Learning-Full-Course\n","!mv PyTorch-for-Deep-Learning-Machine-Learning-Full-Course/going_modular .\n","!mv PyTorch-for-Deep-Learning-Machine-Learning-Full-Course/helper_functions.py .\n","!rm -rf PyTorch-for-Deep-Learning-Machine-Learning-Full-Course"],"metadata":{"id":"Etf4UXzbOBVz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# load libraries\n","import torch\n","import torchvision\n","\n","import matplotlib.pyplot as plt\n","import pathplib\n","\n","from torch import nn\n","from torchvision import transforms, models\n","from torchinfo import summary\n","\n","from going_modular import data_setup, engine, predictions\n","from helper_functions import download_data, set_seeds, plot_loss_curves\n","\n","from PIL import Image\n","from timeit import default_timer as timer\n","from tqdm.auto import tqdm\n","from typing import List, Dict\n","from pathlib import Path"],"metadata":{"id":"GVJixsmWNdCO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# device agnostic code\n","deivce = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","device"],"metadata":{"id":"ejD8x6OGNtp7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# get data\n","data_20 = download_data(source='https://github.com/yhs2773/PyTorch-for-Deep-Learning-Machine-Learning-Full-Course/blob/main/data/pizza_steak_sushi_20_percent.zip',\n","                        destination=\"pizza_steak_sushi_20_percent\")"],"metadata":{"id":"U4go2fg-OfeJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# set directories\n","train_dir = data_20 / \"train\"\n","test_dir = data_20 / \"test\"\n","\n","train_dir, test_dir"],"metadata":{"id":"w_OgJYxdPF2u"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# create model function\n","def create_model(num_classes: int=3,\n","                 seed: int=42,\n","                 is_effnetb2: bool=True):\n","    if is_effnetb2:\n","        weights = models.EfficientNet_B2_Weights.DEFAULT\n","        transforms = weights.transforms()\n","        model = models.efficientnet_b2(weights=weights)\n","\n","        for param in model.parameters():\n","            param.requires_grad = False\n","\n","        torch.manual_seed(seed)\n","        model.classifier = nn.Sequential(\n","            nn.Dropout(0.3, inplace=True),\n","            nn.Linear(in_features=1408, out_features=num_classes)\n","        )\n","    else:\n","        weights = models.ViT_B_16.Weights.DEFAULT\n","        transforms = weights.transforms()\n","        model = models.vit_b_16(weights=weights)\n","\n","        for param in model.parameters():\n","            param.requires_grad = False\n","\n","        torch.manual_seed(seed)\n","        model.heads = nn.Sequential(\n","            nn.Linear(in_features=768,\n","                      out_features=num_classes)\n","        )\n","\n","    return model, transforms"],"metadata":{"id":"5AXfsd9IPXns"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# EffNetB2 model\n","effnetb2, effnetb2_transforms = create_model(num_classes=3,\n","                                             seed=42,\n","                                             is_effnetb2=True)"],"metadata":{"id":"QmaZlM6WQIkt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ViT model\n","vit, vit_transforms = create_model(num_classes=3,\n","                                   seed=42,\n","                                   is_effnetb2=False)"],"metadata":{"id":"stZS28MkQSZR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# create EffNetB2 dataloaders\n","train_dataloader_effnetb2, test_dataloader_effnetb2, class_names = data_setup.create_dataloaders(\n","    train_dir=train_dir,\n","    test_dir=test_dir,\n","    transforms=effnetb2_transforms,\n","    batch_size=32\n",")"],"metadata":{"id":"IoGDLVc0TP_M"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# create ViT dataloaders\n","train_dataloder_vit, test_dataloader_vit, class_names = data_setup.create_dataloaders(\n","    train_dir=train_dir,\n","    test_dir=test_dir,\n","    transforms=vit_transforms,\n","    batch_size=32\n",")"],"metadata":{"id":"P_MaI3umTjg6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 1. Make and time predictions with both feature extractor models on the test dataset using the GPU (`device=\"cuda\"`). Compare the model's prediction times on GPU vs CPU - does this close the gap between them? As in, does making predictions on the GPU make the ViT feature extractor prediction times closer to the EffNetB2 feature extractor prediction times?\n","- You'll find code to do these steps in [section 5. Making predictions with our trained models and timing them](https://www.learnpytorch.io/09_pytorch_model_deployment/#5-making-predictions-with-our-trained-models-and-timing-them) and [section 6. Comparing model results, prediction times and size](https://www.learnpytorch.io/09_pytorch_model_deployment/#6-comparing-model-results-prediction-times-and-size)."],"metadata":{"id":"jji1CI4MgC_1"}},{"cell_type":"code","source":["# get test data paths\n","test_data_paths = list(Path(test_dir).glob(\"*/*.jpg\"))"],"metadata":{"id":"VCtM5PLTX3Hf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# GPU model results\n","effnetb2_results_gpu = predictions.pred_and_store(paths=test_data_paths,\n","                                                  model=effnetb2,\n","                                                  transform=effnetb2_transforms,\n","                                                  class_names=class_names,\n","                                                  device=\"cuda\")\n","\n","vit_results_gpu = predictions.pred_and_store(paths=test_data_paths,\n","                                             model=vit,\n","                                             transform=vit_transforms,\n","                                             class_names=class_names,\n","                                             device=\"cuda\")"],"metadata":{"id":"p5Z8rCCxUHlp"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-Ed70uFPe6jD"},"outputs":[],"source":["# CPU model results\n","effnetb2_results_cpu = predictions.pred_and_store(paths=test_data_paths,\n","                                                  model=effnetb2,\n","                                                  transforms=effnetb2_transforms,\n","                                                  class_names=class_names,\n","                                                  device=\"cpu\")\n","\n","vit_results_cpu = predictions.pred_and_store(paths=test_data_paths,\n","                                             model=vit,\n","                                             transforms=vit_transforms,\n","                                             class_names=class_names,\n","                                             device=\"cpu\")"]},{"cell_type":"markdown","source":["## 2. The ViT feature extractor seems to have more learning capacity (due to more parameters) than EffNetB2, how does it go on the larger 20% split of the entire Food101 dataset?\n","- Train a ViT feature extractor on the 20% Food101 dataset for 5 epochs, just like we did with EffNetB2 in [section 10. Creating FoodVision Big](https://www.learnpytorch.io/09_pytorch_model_deployment/#10-creating-foodvision-big).\n"],"metadata":{"id":"6-3hIiz2gRZC"}},{"cell_type":"code","source":[],"metadata":{"id":"n0pRfQelgZUT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 3. Make predictions across the 20% Food101 test dataset with the ViT feature extractor from exercise 2 and find the \"most wrong\" predictions.\n","- The predictions will be the ones with the highest prediction probability but with the wrong predicted label.\n","- Write a sentence or two about why you think the model got these predictions wrong."],"metadata":{"id":"63ubOyYegZ8b"}},{"cell_type":"code","source":[],"metadata":{"id":"Q4Vyp2VQgcjl"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 4. Evaluate the ViT feature extractor across the whole Food101 test dataset rather than just the 20% version, how does it perform?\n","- Does it beat the original Food101 paper's best result of 56.4% accuracy?"],"metadata":{"id":"xBUOrJw_gema"}},{"cell_type":"code","source":[],"metadata":{"id":"zn0D_h6oggcX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 5. Head to Paperswithcode.com and find the current best performing model on the Food101 dataset.\n","- What model architecture does it use?"],"metadata":{"id":"W2WkCVyggg5h"}},{"cell_type":"code","source":[],"metadata":{"id":"vPjg2UTAgjLG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 6. Write down 1-3 potential failure points of our deployed FoodVision models and what some potential solutions might be.\n","- For example, what happens if someone was to upload a photo that wasn't of food to our FoodVision Mini model?"],"metadata":{"id":"EWYnTOMxgkxp"}},{"cell_type":"code","source":[],"metadata":{"id":"3evAJB7Hgl_9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 7. Pick any dataset from [`torchvision.datasets`](https://pytorch.org/vision/stable/datasets.html) and train a feature extractor model on it using a model from [`torchvision.models`](https://pytorch.org/vision/stable/models.html) (you could use one of the model's we've already created, e.g. EffNetB2 or ViT) for 5 epochs and then deploy your model as a Gradio app to Hugging Face Spaces.\n","- You may want to pick smaller dataset/make a smaller split of it so training doesn't take too long.\n","- I'd love to see your deployed models! So be sure to share them in Discord or on the [course GitHub Discussions page](https://github.com/mrdbourke/pytorch-deep-learning/discussions)."],"metadata":{"id":"H-TJXChKgniN"}},{"cell_type":"code","source":[],"metadata":{"id":"llw4hKDOgxqG"},"execution_count":null,"outputs":[]}]}