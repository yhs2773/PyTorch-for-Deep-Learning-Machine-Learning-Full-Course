{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNBwD9h8hPqNHi3laHP6A1y"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# 09. PyTorch Model Deployment\n","\n","Welcome to Milestone Project 3: PyTorch Model Deployment!\n","\n","We've come a long way with our FoodVision Mini project.\n","\n","But so far our PyTorch models have only been accessible to us.\n","\n","How about we bring FoodVision Mini to life and make it publically accessible?\n","\n","In other words, **we're going to deploy our FoodVision Mini model to the internet as a usable app!**\n","\n","![image0](https://github.com/mrdbourke/pytorch-deep-learning/raw/main/images/09-model-deployment-what-were-doing-demo-trimmed-cropped-small.gif)\n","\n","*Trying out the [deployed version of FoodVision Mini](https://huggingface.co/spaces/mrdbourke/foodvision_mini) (what we're going to build) on my lunch. The model got it right too 🍣!*"],"metadata":{"id":"-bnLWO8Cg2b-"}},{"cell_type":"markdown","source":["## What is machine learning model deployment?\n","\n","**Machine learning model deployment** is the process of making your machine learning model accessible to someone or something else.\n","\n","Someone else is a person who can interact with your model in some way.\n","\n","For example, someone takes a photo on their smartphone of food and then has our FoodVision Mini model classify it into pizza, steak, or sushi.\n","\n","Something else might be another program, app or even another model that interacts with your machine learning model(s).\n","\n","For example, a banking database might rely on a machine learning model making predictions as to whether a transaction is fraudulent or not before transferring funds.\n","\n","Or an operating system may lower its resource consumption based on a machine learning model making predictions on how much power someone generally uses at specific times of day.\n","\n","These use cases can be mixed and matched as well.\n","\n","For example, a Tesla car's computer vision system will interact with the car's route planning program (something else) and then the route planning program will get inputs and feedback from the driver (someone else).\n","\n","![image1](https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/09-what-is-model-deployment-someone-or-something-else.png)\n","\n","*Machine learning model deployment involves making your model available to someone or something else. For example, someone might use your model as part of a food recognition app (such as FoodVision Mini or [Nutrify](https://nutrify.app/)). And something else might be another model or program using your model such as a banking system using a machine learning model to detect if a transaction is fraud or not.*"],"metadata":{"id":"8dv3EBVdhYEt"}},{"cell_type":"markdown","source":["## Why deploy a machine learning model?\n","\n","One of the most important philosophical questions in machine learning is:\n","\n","![image2](https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/09-does-it-exist.jpeg)\n","\n","Deploying a model is as important as training one.\n","\n","Because although you can get a pretty good idea of how your model's going to function by evaluating it on a well-crafted test set or visualizing its results, you never really know how it's going to perform until you release it to the wild.\n","\n","Having people who've never used your model interact with it will often reveal edge cases you never thought of during training.\n","\n","For example, what happens if someone was to upload a photo that *wasn't* of food to our FoodVision Mini model?\n","\n","One solution would be to create another model that first classifies images as \"food\" or \"not food\" and passes the target image through that model first (this is what [Nutrify](https://nutrify.app/) does).\n","\n","Then if the image is of \"food\" it goes to our FoodVision Mini model and gets classified into pizza, steak, or sushi.\n","\n","And if it's \"not food\", a message is displayed.\n","\n","But what if these predictions were wrong?\n","\n","What happens then?\n","\n","You can see how these questions could keep going.\n","\n","Thus this highlights the importance of model deployment: it helps you figure out errors in your model that aren't obvious during training/testing.\n","\n","![image3](https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/09-pytorch-workflow-with-deployment.png)\n","\n","*We covered a PyTorch workflow back in [01. PyTorch Workflow](https://www.learnpytorch.io/01_pytorch_workflow/). But once you've got a good model, deployment is a good next step. Monitoring involves seeing how your model goes on the most important data split: data from the real world. For more resources on deployment and monitoring see [PyTorch Extra Resources](https://www.learnpytorch.io/pytorch_extra_resources/#resources-for-machine-learning-and-deep-learning-engineering).*"],"metadata":{"id":"wBCI4tlOjX1s"}},{"cell_type":"markdown","source":["## Different types of machine learning model deployment\n","\n","Whole books could be written on the different types of machine learning model deployment (and many good ones are listed in [PyTorch Extra Resources](https://www.learnpytorch.io/pytorch_extra_resources/#resources-for-machine-learning-and-deep-learning-engineering)).\n","\n","And the field is still developing in terms of best practices.\n","\n","But I like to start with the question:\n","\n","> \"What is the most ideal scenario for my machine learning model to be used?\"\n","\n","And then work backwards from there.\n","\n","Of course, you may not know this ahead of time. But you're smart enough to imagine such things.\n","\n","In the case of FoodVision Mini, our ideal scenario might be:\n","\n","- Someone takes a photo on a mobile device (through an app or web browser).\n","- The prediction comes back fast.\n","\n","Easy.\n","\n","So we've got two main criteria:\n","\n","1. The model should work on a mobile device (this means there will be some computing constraints).\n","2. The model should make predictions *fast* (because a slow app is a boring app).\n","\n","And of course, depending on your use case, your requirements may vary.\n","\n","You may notice the above two points break down into another two questions:\n","\n","1. **Where's it going to go?** - As in, where is it going to be stored?\n","2. **How's it going to function?** - As in, does it return predictions immediately? Or do they come later?\n","\n","![image4](https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/09-deployment-questions-to-ask.png)\n","\n","*When starting to deploy machine learning models, it's helpful to start by asking what's the most ideal use case and then work backwards from there, asking where the model's going to go and then how it's going to function.*"],"metadata":{"id":"lVr56sNMk2D6"}},{"cell_type":"markdown","source":["## Where's it going to go?\n","\n","When you deploy your machine learning model, where does it live?\n","\n","The main debate here is usually on-device (also called edge/in the browser) or on the cloud (a computer/server that isn't the *actual* device someone/something calls the model from).\n","\n","Both have their pros and cons.\n","\n","**Deployment location**:\n","- **On-device (edge/in the browser)**:\n","    - **Pros**\n","        - Can be very fast (since no data leaves the device)\n","        - Privacy preserving (again no data has to leave the device)\n","        - No internet connection required (sometimes)\n","    - **Cons**\n","        - Limited compute power (larger models take longer to run)\n","        - Limited storage space (smaller model size required)\n","        - Device-specific skills often required\n","- **On cloud**:\n","    - **Pros**\n","        - Near unlimited compute power (can scale up when needed)\n","        - Can deploy one model and use it everywhere (via API)\n","        - Links into the existing cloud ecosystem\n","    - **Cons**\n","        - Costs can get out of hand (if proper scaling limits aren't enforced)\n","        - Predictions can be slower due to data having to leave the device and predictions having to come back (network latency)\n","        - Data has to leave the device (this may cause privacy concerns)\n","\n","There are more details to these but I've left resources in the [extra-curriculum](https://www.learnpytorch.io/09_pytorch_model_deployment/#extra-curriculum) to learn more.\n","\n","Let's give an example.\n","\n","If we're deploying FoodVision Mini as an app, we want it to perform well and fast.\n","\n","So which model would we prefer?\n","\n","1. A model on-device that performs at 95% accuracy with an inference time (latency) of one second per prediction.\n","2. A model on the cloud that performs at 98% accuracy with an inference time of 10 seconds per prediction (bigger, better model but takes longer to compute).\n","\n","I've made these numbers up, but they showcase a potential difference between on-device and on the cloud.\n","\n","Option 1 could potentially be a smaller less performant model that runs fast because it's able to fit on a mobile device.\n","\n","Option 2 could potentially be a larger more performant model that requires more computing and storage but it takes a bit longer to run because we have to send data off the device and get it back (so even though the actual prediction might be fast, the network time and data transfer has to factored in).\n","\n","For FoodVision Mini, we'd likely prefer option 1, because the small hit in performance is far outweighed by the faster inference speed.\n","\n","![image5](https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/09-model-deployment-on-device-vs-cloud.png)\n","\n","*In the case of a Tesla car's computer vision system, which would be better? A smaller model that performs well on device (model is on the car) or a larger model that performs better that's on the cloud? In this case, you'd much prefer the model being on the car. The extra network time it would take for data to go from the car to the cloud and then back to the car just wouldn't be worth it (or potentially even possible with poor signal areas).*\n","\n","> **Note**: For a full example of seeing what it's like to deploy a PyTorch model to an edge device, see the [PyTorch tutorial on achieving real-time inference (30fps+)](https://pytorch.org/tutorials/intermediate/realtime_rpi.html) with a computer vision model on a Raspberry Pi."],"metadata":{"id":"MtNyldQPmCF6"}},{"cell_type":"markdown","source":["## How's it going to function?\n","\n","Back to the ideal use case, when you deploy your machine learning model, how should it work?\n","\n","As in, would you like predictions returned immediately?\n","\n","Or is it okay for them to happen later?\n","\n","These two scenarios are generally referred to as:\n","\n","- **Online (real-time)**: Predicitions/inference happen **immediately**. For example, someone uploads an image, the image gets transformed and predictions are returned or someone makes a purchase and the transaction is verified to be non-fraudulent by a model so the purchase can go through.\n","- **Offline (batch)**: Predictions/inference happen **periodically**. For example, a photo application sorts your images into different categories (such as beach, mealtime, family, and friends) whilst your mobile device is plugged into charge.\n","\n","> **Note**: \"Batch\" refers to inference being performed on multiple samples at a time. However, to add a little confusion, batch processing can happen immediately/online (multiple images being classified at once) and/or offline (multiple images being predicted/trained on at once).\n","\n","The main difference between each is: predictions are made immediately or periodically.\n","\n","Periodically can have a varying timescale too, from every few seconds to every few hours or days.\n","\n","And you can mix and match the two.\n","\n","In the case of FoodVision Mini, we'd want our inference pipeline to happen online (real-time), so when someone uploads an image of pizza, steak, or sushi, the prediction results are returned immediately (any slower than real-time would make a boring experience).\n","\n","But for our training pipeline, it's okay for it to happen in a batch (offline) fashion, which is what we've been doing throughout the previous chapters."],"metadata":{"id":"x2P4pxTZoU9q"}},{"cell_type":"markdown","source":["## Ways to deploy a machine learning model\n","\n","We've discussed a couple of options for deploying machine learning models (on-device and cloud).\n","\n","And each of these will have their specific requirements:\n","\n","| Tool/resource | Deployment type |\n","|--|--|\n","| [Google's ML Kit](https://developers.google.com/ml-kit) |\tOn-device (Android and iOS) |\n","| [Apple's Core ML](https://developer.apple.com/documentation/coreml) and [`coremltools` Python package](https://coremltools.readme.io/docs) |\tOn-device (all Apple devices) |\n","| [Amazon Web Service's (AWS) Sagemaker](https://aws.amazon.com/sagemaker/) |\tCloud |\n","| [Google Cloud's Vertex AI](https://cloud.google.com/vertex-ai) |\tCloud |\n","| [Microsoft's Azure Machine Learning](https://azure.microsoft.com/en-au/services/machine-learning/) |\tCloud |\n","| [Hugging Face Spaces](https://huggingface.co/spaces) |\tCloud |\n","| API with [FastAPI](https://fastapi.tiangolo.com/) |\tCloud/self-hosted server |\n","| API with [TorchServe](https://pytorch.org/serve/) |\tCloud/self-hosted server |\n","| [ONNX (Open Neural Network Exchange)](https://onnx.ai/index.html) | Many/general |\n","| Many more...\t|\n","\n","> **Note**: An [application programming interface (API)](https://en.wikipedia.org/wiki/API) is a way for two (or more) computer programs to interact with each other. For example, if your model was deployed as API, you would be able to write a program that could send data to it and then receive predictions back.\n","\n","Which option you choose will be highly dependent on what you're building/who you're working with.\n","\n","But with so many options, it can be very intimidating.\n","\n","So best to start small and keep it simple.\n","\n","And one of the best ways to do so is by turning your machine learning model into a demo app with [Gradio](https://gradio.app/) and then deploying it on Hugging Face Spaces.\n","\n","We'll be doing just that with FoodVision Mini later on.\n","\n","![image6](https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/09-tools-and-places-to-deploy-ml-models.png)\n","\n","*A handful of places and tools to host and deploy machine learning models. There are plenty I've missed so if you'd like to add more, please leave a [discussion on GitHub](https://github.com/mrdbourke/pytorch-deep-learning/discussions).*"],"metadata":{"id":"tg0HFpV_pgKL"}},{"cell_type":"markdown","source":["## What we're going to cover\n","\n","Enough talking about deploying a machine learning model.\n","\n","Let's become machine learning engineers and deploy one.\n","\n","Our goal is to deploy our FoodVision Model via a demo Gradio app with the following metrics:\n","\n","- **Performance**: 95%+ accuracy.\n","- **Speed**: real-time inference of 30FPS+ (each prediction has a latency of lower than ~0.03s).\n","\n","We'll start by running an experiment to compare our best two models so far: EffNetB2 and ViT feature extractors.\n","\n","Then we'll deploy the one which performs closest to our goal metrics.\n","\n","Finally, we'll finish with a (BIG) surprise bonus."],"metadata":{"id":"IVwlEiDqrEze"}},{"cell_type":"markdown","source":["## 0. Getting setup\n","\n","As we've done previously, let's make sure we've got all of the modules we'll need for this section.\n","\n","We'll import the Python scripts (such as `data_setup.py` and `engine.py`) we created in [05. PyTorch Going Modular](https://www.learnpytorch.io/05_pytorch_going_modular/).\n","\n","To do so, we'll download the [`going_modular`](https://github.com/yhs2773/PyTorch-for-Deep-Learning-Machine-Learning-Full-Course/tree/main/going_modular) directory from the [`pytorch-deep-learning` repository](https://github.com/yhs2773/PyTorch-for-Deep-Learning-Machine-Learning-Full-Course) (if we don't already have it).\n","\n","We'll also get the [`torchinfo`](https://github.com/TylerYep/torchinfo) package if it's not available.\n","\n","`torchinfo` will help later on to give us a visual representation of our model.\n","\n","And since later on we'll be using the `torchvision` v0.13 package (available as of July 2022), we'll make sure we've got the latest versions.\n","\n","> **Note**: If you're using Google Colab, and you don't have a GPU turned on yet, it's now time to turn one on via `Runtime -> Change runtime type -> Hardware accelerator -> GPU`."],"metadata":{"id":"q-IXp5J3riSF"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"DadQ3Rl-gzlX"},"outputs":[],"source":[]}]}