{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOxOQLym15aC1+11RvkL0xU"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# 09. PyTorch Model Deployment\n","\n","Welcome to Milestone Project 3: PyTorch Model Deployment!\n","\n","We've come a long way with our FoodVision Mini project.\n","\n","But so far our PyTorch models have only been accessible to us.\n","\n","How about we bring FoodVision Mini to life and make it publically accessible?\n","\n","In other words, **we're going to deploy our FoodVision Mini model to the internet as a usable app!**\n","\n","![image0](https://github.com/mrdbourke/pytorch-deep-learning/raw/main/images/09-model-deployment-what-were-doing-demo-trimmed-cropped-small.gif)\n","\n","*Trying out the [deployed version of FoodVision Mini](https://huggingface.co/spaces/mrdbourke/foodvision_mini) (what we're going to build) on my lunch. The model got it right too ðŸ£!*"],"metadata":{"id":"-bnLWO8Cg2b-"}},{"cell_type":"markdown","source":["## What is machine learning model deployment?\n","\n","**Machine learning model deployment** is the process of making your machine learning model accessible to someone or something else.\n","\n","Someone else is a person who can interact with your model in some way.\n","\n","For example, someone takes a photo on their smartphone of food and then has our FoodVision Mini model classify it into pizza, steak, or sushi.\n","\n","Something else might be another program, app or even another model that interacts with your machine learning model(s).\n","\n","For example, a banking database might rely on a machine learning model making predictions as to whether a transaction is fraudulent or not before transferring funds.\n","\n","Or an operating system may lower its resource consumption based on a machine learning model making predictions on how much power someone generally uses at specific times of day.\n","\n","These use cases can be mixed and matched as well.\n","\n","For example, a Tesla car's computer vision system will interact with the car's route planning program (something else) and then the route planning program will get inputs and feedback from the driver (someone else).\n","\n","![image1](https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/09-what-is-model-deployment-someone-or-something-else.png)\n","\n","*Machine learning model deployment involves making your model available to someone or something else. For example, someone might use your model as part of a food recognition app (such as FoodVision Mini or [Nutrify](https://nutrify.app/)). And something else might be another model or program using your model such as a banking system using a machine learning model to detect if a transaction is fraud or not.*"],"metadata":{"id":"8dv3EBVdhYEt"}},{"cell_type":"markdown","source":["## Why deploy a machine learning model?\n","\n","One of the most important philosophical questions in machine learning is:\n","\n","![image2](https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/09-does-it-exist.jpeg)\n","\n","Deploying a model is as important as training one.\n","\n","Because although you can get a pretty good idea of how your model's going to function by evaluating it on a well-crafted test set or visualizing its results, you never really know how it's going to perform until you release it to the wild.\n","\n","Having people who've never used your model interact with it will often reveal edge cases you never thought of during training.\n","\n","For example, what happens if someone was to upload a photo that *wasn't* of food to our FoodVision Mini model?\n","\n","One solution would be to create another model that first classifies images as \"food\" or \"not food\" and passes the target image through that model first (this is what [Nutrify](https://nutrify.app/) does).\n","\n","Then if the image is of \"food\" it goes to our FoodVision Mini model and gets classified into pizza, steak, or sushi.\n","\n","And if it's \"not food\", a message is displayed.\n","\n","But what if these predictions were wrong?\n","\n","What happens then?\n","\n","You can see how these questions could keep going.\n","\n","Thus this highlights the importance of model deployment: it helps you figure out errors in your model that aren't obvious during training/testing.\n","\n","![image3](https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/09-pytorch-workflow-with-deployment.png)\n","\n","*We covered a PyTorch workflow back in [01. PyTorch Workflow](https://www.learnpytorch.io/01_pytorch_workflow/). But once you've got a good model, deployment is a good next step. Monitoring involves seeing how your model goes on the most important data split: data from the real world. For more resources on deployment and monitoring see [PyTorch Extra Resources](https://www.learnpytorch.io/pytorch_extra_resources/#resources-for-machine-learning-and-deep-learning-engineering).*"],"metadata":{"id":"wBCI4tlOjX1s"}},{"cell_type":"markdown","source":["## Different types of machine learning model deployment\n","\n","Whole books could be written on the different types of machine learning model deployment (and many good ones are listed in [PyTorch Extra Resources](https://www.learnpytorch.io/pytorch_extra_resources/#resources-for-machine-learning-and-deep-learning-engineering)).\n","\n","And the field is still developing in terms of best practices.\n","\n","But I like to start with the question:\n","\n","> \"What is the most ideal scenario for my machine learning model to be used?\"\n","\n","And then work backwards from there.\n","\n","Of course, you may not know this ahead of time. But you're smart enough to imagine such things.\n","\n","In the case of FoodVision Mini, our ideal scenario might be:\n","\n","- Someone takes a photo on a mobile device (through an app or web browser).\n","- The prediction comes back fast.\n","\n","Easy.\n","\n","So we've got two main criteria:\n","\n","1. The model should work on a mobile device (this means there will be some computing constraints).\n","2. The model should make predictions *fast* (because a slow app is a boring app).\n","\n","And of course, depending on your use case, your requirements may vary.\n","\n","You may notice the above two points break down into another two questions:\n","\n","1. **Where's it going to go?** - As in, where is it going to be stored?\n","2. **How's it going to function?** - As in, does it return predictions immediately? Or do they come later?\n","\n","![image4](https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/09-deployment-questions-to-ask.png)\n","\n","*When starting to deploy machine learning models, it's helpful to start by asking what's the most ideal use case and then work backwards from there, asking where the model's going to go and then how it's going to function.*"],"metadata":{"id":"lVr56sNMk2D6"}},{"cell_type":"markdown","source":["## Where's it going to go?\n","\n","When you deploy your machine learning model, where does it live?\n","\n","The main debate here is usually on-device (also called edge/in the browser) or on the cloud (a computer/server that isn't the *actual* device someone/something calls the model from).\n","\n","Both have their pros and cons.\n","\n","**Deployment location**:\n","- **On-device (edge/in the browser)**:\n","    - **Pros**\n","        - Can be very fast (since no data leaves the device)\n","        - Privacy preserving (again no data has to leave the device)\n","        - No internet connection required (sometimes)\n","    - **Cons**\n","        - Limited compute power (larger models take longer to run)\n","        - Limited storage space (smaller model size required)\n","        - Device-specific skills often required\n","- **On cloud**:\n","    - **Pros**\n","        - Near unlimited compute power (can scale up when needed)\n","        - Can deploy one model and use it everywhere (via API)\n","        - Links into the existing cloud ecosystem\n","    - **Cons**\n","        - Costs can get out of hand (if proper scaling limits aren't enforced)\n","        - Predictions can be slower due to data having to leave the device and predictions having to come back (network latency)\n","        - Data has to leave the device (this may cause privacy concerns)\n","\n","There are more details to these but I've left resources in the [extra-curriculum](https://www.learnpytorch.io/09_pytorch_model_deployment/#extra-curriculum) to learn more.\n","\n","Let's give an example.\n","\n","If we're deploying FoodVision Mini as an app, we want it to perform well and fast.\n","\n","So which model would we prefer?\n","\n","1. A model on-device that performs at 95% accuracy with an inference time (latency) of one second per prediction.\n","2. A model on the cloud that performs at 98% accuracy with an inference time of 10 seconds per prediction (bigger, better model but takes longer to compute).\n","\n","I've made these numbers up, but they showcase a potential difference between on-device and on the cloud.\n","\n","Option 1 could potentially be a smaller less performant model that runs fast because it's able to fit on a mobile device.\n","\n","Option 2 could potentially be a larger more performant model that requires more computing and storage but it takes a bit longer to run because we have to send data off the device and get it back (so even though the actual prediction might be fast, the network time and data transfer has to factored in).\n","\n","For FoodVision Mini, we'd likely prefer option 1, because the small hit in performance is far outweighed by the faster inference speed.\n","\n","![image5](https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/09-model-deployment-on-device-vs-cloud.png)\n","\n","*In the case of a Tesla car's computer vision system, which would be better? A smaller model that performs well on device (model is on the car) or a larger model that performs better that's on the cloud? In this case, you'd much prefer the model being on the car. The extra network time it would take for data to go from the car to the cloud and then back to the car just wouldn't be worth it (or potentially even possible with poor signal areas).*\n","\n","> **Note**: For a full example of seeing what it's like to deploy a PyTorch model to an edge device, see the [PyTorch tutorial on achieving real-time inference (30fps+)](https://pytorch.org/tutorials/intermediate/realtime_rpi.html) with a computer vision model on a Raspberry Pi."],"metadata":{"id":"MtNyldQPmCF6"}},{"cell_type":"markdown","source":["## How's it going to function?\n","\n","Back to the ideal use case, when you deploy your machine learning model, how should it work?\n","\n","As in, would you like predictions returned immediately?\n","\n","Or is it okay for them to happen later?\n","\n","These two scenarios are generally referred to as:\n","\n","- **Online (real-time)**: Predicitions/inference happen **immediately**. For example, someone uploads an image, the image gets transformed and predictions are returned or someone makes a purchase and the transaction is verified to be non-fraudulent by a model so the purchase can go through.\n","- **Offline (batch)**: Predictions/inference happen **periodically**. For example, a photo application sorts your images into different categories (such as beach, mealtime, family, and friends) whilst your mobile device is plugged into charge.\n","\n","> **Note**: \"Batch\" refers to inference being performed on multiple samples at a time. However, to add a little confusion, batch processing can happen immediately/online (multiple images being classified at once) and/or offline (multiple images being predicted/trained on at once).\n","\n","The main difference between each is: predictions are made immediately or periodically.\n","\n","Periodically can have a varying timescale too, from every few seconds to every few hours or days.\n","\n","And you can mix and match the two.\n","\n","In the case of FoodVision Mini, we'd want our inference pipeline to happen online (real-time), so when someone uploads an image of pizza, steak, or sushi, the prediction results are returned immediately (any slower than real-time would make a boring experience).\n","\n","But for our training pipeline, it's okay for it to happen in a batch (offline) fashion, which is what we've been doing throughout the previous chapters."],"metadata":{"id":"x2P4pxTZoU9q"}},{"cell_type":"markdown","source":["## Ways to deploy a machine learning model\n","\n","We've discussed a couple of options for deploying machine learning models (on-device and cloud).\n","\n","And each of these will have their specific requirements:\n","\n","| Tool/resource | Deployment type |\n","|--|--|\n","| [Google's ML Kit](https://developers.google.com/ml-kit) |\tOn-device (Android and iOS) |\n","| [Apple's Core ML](https://developer.apple.com/documentation/coreml) and [`coremltools` Python package](https://coremltools.readme.io/docs) |\tOn-device (all Apple devices) |\n","| [Amazon Web Service's (AWS) Sagemaker](https://aws.amazon.com/sagemaker/) |\tCloud |\n","| [Google Cloud's Vertex AI](https://cloud.google.com/vertex-ai) |\tCloud |\n","| [Microsoft's Azure Machine Learning](https://azure.microsoft.com/en-au/services/machine-learning/) |\tCloud |\n","| [Hugging Face Spaces](https://huggingface.co/spaces) |\tCloud |\n","| API with [FastAPI](https://fastapi.tiangolo.com/) |\tCloud/self-hosted server |\n","| API with [TorchServe](https://pytorch.org/serve/) |\tCloud/self-hosted server |\n","| [ONNX (Open Neural Network Exchange)](https://onnx.ai/index.html) | Many/general |\n","| Many more...\t|\n","\n","> **Note**: An [application programming interface (API)](https://en.wikipedia.org/wiki/API) is a way for two (or more) computer programs to interact with each other. For example, if your model was deployed as API, you would be able to write a program that could send data to it and then receive predictions back.\n","\n","Which option you choose will be highly dependent on what you're building/who you're working with.\n","\n","But with so many options, it can be very intimidating.\n","\n","So best to start small and keep it simple.\n","\n","And one of the best ways to do so is by turning your machine learning model into a demo app with [Gradio](https://gradio.app/) and then deploying it on Hugging Face Spaces.\n","\n","We'll be doing just that with FoodVision Mini later on.\n","\n","![image6](https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/09-tools-and-places-to-deploy-ml-models.png)\n","\n","*A handful of places and tools to host and deploy machine learning models. There are plenty I've missed so if you'd like to add more, please leave a [discussion on GitHub](https://github.com/mrdbourke/pytorch-deep-learning/discussions).*"],"metadata":{"id":"tg0HFpV_pgKL"}},{"cell_type":"markdown","source":["## What we're going to cover\n","\n","Enough talking about deploying a machine learning model.\n","\n","Let's become machine learning engineers and deploy one.\n","\n","Our goal is to deploy our FoodVision Model via a demo Gradio app with the following metrics:\n","\n","- **Performance**: 95%+ accuracy.\n","- **Speed**: real-time inference of 30FPS+ (each prediction has a latency of lower than ~0.03s).\n","\n","We'll start by running an experiment to compare our best two models so far: EffNetB2 and ViT feature extractors.\n","\n","Then we'll deploy the one which performs closest to our goal metrics.\n","\n","Finally, we'll finish with a (BIG) surprise bonus."],"metadata":{"id":"IVwlEiDqrEze"}},{"cell_type":"markdown","source":["## 0. Getting setup\n","\n","As we've done previously, let's make sure we've got all of the modules we'll need for this section.\n","\n","We'll import the Python scripts (such as `data_setup.py` and `engine.py`) we created in [05. PyTorch Going Modular](https://www.learnpytorch.io/05_pytorch_going_modular/).\n","\n","To do so, we'll download the [`going_modular`](https://github.com/yhs2773/PyTorch-for-Deep-Learning-Machine-Learning-Full-Course/tree/main/going_modular) directory from the [`pytorch-deep-learning` repository](https://github.com/yhs2773/PyTorch-for-Deep-Learning-Machine-Learning-Full-Course) (if we don't already have it).\n","\n","We'll also get the [`torchinfo`](https://github.com/TylerYep/torchinfo) package if it's not available.\n","\n","`torchinfo` will help later on to give us a visual representation of our model.\n","\n","And since later on we'll be using the `torchvision` v0.13 package (available as of July 2022), we'll make sure we've got the latest versions.\n","\n","> **Note**: If you're using Google Colab, and you don't have a GPU turned on yet, it's now time to turn one on via `Runtime -> Change runtime type -> Hardware accelerator -> GPU`."],"metadata":{"id":"q-IXp5J3riSF"}},{"cell_type":"code","execution_count":1,"metadata":{"id":"DadQ3Rl-gzlX","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1689325341282,"user_tz":-540,"elapsed":5503,"user":{"displayName":"Hee Seong Yang","userId":"11765788859671095501"}},"outputId":"26306c46-68f0-4eca-a2a7-cefa771e0ee3"},"outputs":[{"output_type":"stream","name":"stdout","text":["torch version: 2.0.1+cu118\n","torchvision version: 0.15.2+cu118\n"]}],"source":["import torch\n","import torchvision\n","print(f\"torch version: {torch.__version__}\")\n","print(f\"torchvision version: {torchvision.__version__}\")"]},{"cell_type":"markdown","source":["> **Note**: If you're using Google Colab and the cell above starts to install various software packages, you may have to restart your runtime after running the above cell. After restarting, you can run the cell again and verify you've got the right versions of `torch` and `torchvision`.\n","\n","Now we'll continue with the regular imports, setting up device agnostic code and this time we'll also get the [`helper_functions.py`](https://github.com/yhs2773/PyTorch-for-Deep-Learning-Machine-Learning-Full-Course/blob/main/helper_functions.py) script from GitHub.\n","\n","The `helper_functions.py` script contains several functions we created in previous sections:\n","\n","- `set_seeds()` to set the random seeds (created in [07. PyTorch Experiment Tracking section 0](https://www.learnpytorch.io/07_pytorch_experiment_tracking/#create-a-helper-function-to-set-seeds)).\n","- `download_data()` to download a data source given a link (created in [07. PyTorch Experiment Tracking section 1](https://www.learnpytorch.io/07_pytorch_experiment_tracking/#1-get-data)).\n","- `plot_loss_curves()` to inspect our model's training results (created in [04. PyTorch Custom Datasets section 7.8](https://www.learnpytorch.io/04_pytorch_custom_datasets/#78-plot-the-loss-curves-of-model-0))\n","\n","> **Note**: It may be a better idea for many of the functions in the `helper_functions.py` script to be merged into `going_modular/utils.py`, perhaps that's an extension you'd like to try."],"metadata":{"id":"eqdStTflouv2"}},{"cell_type":"code","source":["# Import other libraries\n","import matplotlib.pyplot as plt\n","\n","from torch import nn\n","from torchvision import transforms\n","\n","# Try to get torchinfo\n","try:\n","    from torchinfo import summary\n","except:\n","    print(\"Install torchinfo\")\n","    !pip install -qq torchinfo\n","    from torchinfo import summary\n","\n","# Try to import the going_modular directory\n","try:\n","    from going_modular import data_setup, engine\n","    from helper_functions import download_data, set_seeds, plot_loss_curves\n","except:\n","    print(\"Get scripts\")\n","    !git clone https://github.com/yhs2773/PyTorch-for-Deep-Learning-Machine-Learning-Full-Course\n","    !mv PyTorch-for-Deep-Learning-Machine-Learning-Full-Course/going_modular .\n","    !mv PyTorch-for-Deep-Learning-Machine-Learning-Full-Course/helper_functions.py .\n","    !rm -rf PyTorch-for-Deep-Learning-Machine-Learning-Full-Course\n","    from going_modular import data_setup, engine\n","    from helper_functions import download_data, set_seeds, plot_loss_curves"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nfUgkUiyoru3","executionInfo":{"status":"ok","timestamp":1689325868902,"user_tz":-540,"elapsed":5956,"user":{"displayName":"Hee Seong Yang","userId":"11765788859671095501"}},"outputId":"c1a3f779-34ba-4a50-e517-19bf2fc60527"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Get scripts\n","Cloning into 'PyTorch-for-Deep-Learning-Machine-Learning-Full-Course'...\n","remote: Enumerating objects: 181, done.\u001b[K\n","remote: Counting objects: 100% (125/125), done.\u001b[K\n","remote: Compressing objects: 100% (98/98), done.\u001b[K\n","remote: Total 181 (delta 58), reused 66 (delta 27), pack-reused 56\u001b[K\n","Receiving objects: 100% (181/181), 56.48 MiB | 16.21 MiB/s, done.\n","Resolving deltas: 100% (82/82), done.\n"]}]},{"cell_type":"code","source":["# Setup device agnostic code\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","device"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"nkqPb5ZDqoku","executionInfo":{"status":"ok","timestamp":1689325887381,"user_tz":-540,"elapsed":8,"user":{"displayName":"Hee Seong Yang","userId":"11765788859671095501"}},"outputId":"4ec2bf7b-8420-4451-dd6a-eacd2294e34e"},"execution_count":4,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'cpu'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":4}]},{"cell_type":"markdown","source":["## 1. Getting data\n","\n","We left off in [08. PyTorch Paper Replicating](https://www.learnpytorch.io/08_pytorch_paper_replicating/#106-save-feature-extractor-vit-model-and-check-file-size) comparing our own Vision Transformer (ViT) feature extractor model to the EfficientNetB2 (EffNetB2) feature extractor model we created in [07. PyTorch Experiment Tracking](https://www.learnpytorch.io/07_pytorch_experiment_tracking/#9-load-in-the-best-model-and-make-predictions-with-it).\n","\n","And we found that there was a slight difference in the comparison.\n","\n","The EffNetB2 model was trained on 20% of the pizza, steak, and sushi data from Food101 whereas the ViT model was trained on 10%.\n","\n","Since our goal is to deploy the best model for our FoodVision Mini problem, let's start by downloading the [20% pizza, steak, and sushi dataset](https://github.com/mrdbourke/pytorch-deep-learning/blob/main/data/pizza_steak_sushi_20_percent.zip) and train an EffNetB2 feature extractor and ViT feature extractor on it and then compare the two models.\n","\n","This way we'll be comparing apples to apples (one model trained on a dataset to another model trained on the same dataset).\n","\n","> **Note**: We're downloading a sample of the entire [Food101 dataset](https://pytorch.org/vision/main/generated/torchvision.datasets.Food101.html#food101) (101 food classes with 1,000 images each). More specifically, 20% refers to 20% of images from the pizza, steak, and sushi classes selected at random. You can see how this dataset was created in [`extras/04_custom_data_creation.ipynb`](https://github.com/mrdbourke/pytorch-deep-learning/blob/main/extras/04_custom_data_creation.ipynb) and more details in [04. PyTorch Custom Datasets section 1](https://www.learnpytorch.io/04_pytorch_custom_datasets/#1-get-data).\n","\n","We can download the data using the `download_data()` function we created in [07. PyTorch Experiment Tracking section 1](https://www.learnpytorch.io/07_pytorch_experiment_tracking/#1-get-data) from `helper_functions.py`."],"metadata":{"id":"H6_UakTVq3PR"}},{"cell_type":"code","source":["# Download data from GitHub\n","data_20_percent_path = download_data(source=\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi_20_percent.zip\",\n","                                     destination=\"pizza_steak_sushi_20_percent\")\n","\n","data_20_percent_path"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yZ_YVLJzq1tH","executionInfo":{"status":"ok","timestamp":1689326347254,"user_tz":-540,"elapsed":2985,"user":{"displayName":"Hee Seong Yang","userId":"11765788859671095501"}},"outputId":"ebcc01cb-4717-4b76-e09a-48ec54cde08a"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["[INFO] Did not find data/pizza_steak_sushi_20_percent directory, creating one...\n","[INFO] Downloading pizza_steak_sushi_20_percent.zip from https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi_20_percent.zip...\n","[INFO] Unzipping pizza_steak_sushi_20_percent.zip data...\n"]},{"output_type":"execute_result","data":{"text/plain":["PosixPath('data/pizza_steak_sushi_20_percent')"]},"metadata":{},"execution_count":5}]},{"cell_type":"code","source":["# Setup directory paths to train and test images\n","train_dir = data_20_percent_path / \"train\"\n","test_dir = data_20_percent_path / \"test\"\n","\n","train_dir, test_dir"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lGAYY2tEslQc","executionInfo":{"status":"ok","timestamp":1689326392016,"user_tz":-540,"elapsed":735,"user":{"displayName":"Hee Seong Yang","userId":"11765788859671095501"}},"outputId":"da3771fe-48c9-438f-b171-1f10b1e9da6a"},"execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(PosixPath('data/pizza_steak_sushi_20_percent/train'),\n"," PosixPath('data/pizza_steak_sushi_20_percent/test'))"]},"metadata":{},"execution_count":6}]},{"cell_type":"markdown","source":["## 2. FoodVision Mini model deployment experiment outline\n","\n","The ideal deployed model FoodVision Mini performs well and fast.\n","\n","We'd like our model to perform as close to real-time as possible.\n","\n","Real-time in this case is ~30FPS (frames per second) because that's [about how fast the human eye can see](https://www.healthline.com/health/human-eye-fps) (there is debate on this but let's just use ~30FPS as our benchmark).\n","\n","And for classifying three different classes (pizza, steak, and sushi), we'd like a model that performs at 95%+ accuracy.\n","\n","Of course, higher accuracy would be nice but this might sacrifice speed.\n","\n","So our goals are:\n","\n","- **Performance** - A model that performs at 95%+ accuracy.\n","- **Speed** - A model that can classify an image at ~30FPS (0.03 seconds inference time per image, also known as latency).\n","\n","![image7](https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/09-model-deployments-speed-vs-inference.png)\n","\n","*FoodVision Mini deployment goals. We'd like a fast-predicting well-performing model (because a slow app is boring).*\n","\n","We'll emphasize speed, meaning, we'd prefer a model performing at 90%+ accuracy at ~30FPS than a model performing 95%+ accuracy at 10FPS.\n","\n","To try and achieve these results, let's bring in our best performing models from the previous sections:\n","\n","1. **EffNetB2 feature extractor** (EffNetB2 for short) - originally created in [07. PyTorch Experiment Tracking section 7.5](https://www.learnpytorch.io/07_pytorch_experiment_tracking/#75-create-feature-extractor-models) using [`torchvision.models.efficientnet_b2()`](https://pytorch.org/vision/stable/models/generated/torchvision.models.efficientnet_b2.html#efficientnet-b2) with adjusted `classifier` layers.\n","\n","2. **ViT-B/16 feature extractor** (ViT for short) - created in [08. PyTorch Paper Replicating section 10](https://www.learnpytorch.io/08_pytorch_paper_replicating/#10-using-a-pretrained-vit-from-torchvisionmodels-on-the-same-dataset) using [`torchvision.models.vit_b_16()`](https://pytorch.org/vision/stable/models/generated/torchvision.models.vit_b_16.html#vit-b-16) with adjusted `head` layers.\n","    - **Note**: ViT-B/16 stands for \"Vision Transformer Base, patch size 16\".\n","\n","![image8](https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/09-model-deployment-two-experiments.png)\n","\n","> **Note**: A \"feature extractor model\" often starts with a model that has been pretrained on a dataset similar to your problem. The pretrained model's base layers are often left frozen (the pretrained patterns/weights stay the same) whilst some of the top (or classifier/classification head) layers get customized to your problem by training on your data. We covered the concept of a feature extractor model in [06. PyTorch Transfer Learning section 3.4](https://www.learnpytorch.io/06_pytorch_transfer_learning/#34-freezing-the-base-model-and-changing-the-output-layer-to-suit-our-needs)."],"metadata":{"id":"DbIizNSHs6wk"}},{"cell_type":"markdown","source":["## 3. Creating an EffNetB2 feature extractor\n","\n","We first created an EffNetB2 feature extractor model in [07. PyTorch Experiment Tracking section 7.5](https://www.learnpytorch.io/07_pytorch_experiment_tracking/#75-create-feature-extractor-models).\n","\n","And by the end of that section, we saw it performed very well.\n","\n","So let's now recreate it here so we can compare its results to a ViT feature extractor trained on the same data.\n","\n","To do so we can:\n","\n","1. Setup the pretrained weights as [`weights=torchvision.models.EfficientNet_B2_Weights.DEFAULT`](https://pytorch.org/vision/stable/models/generated/torchvision.models.efficientnet_b2.html#torchvision.models.EfficientNet_B2_Weights), where \"`DEFAULT`\" means \"best currently available\" (or could use `weights=\"DEFAULT\"`).\n","2. Get the pretrained model image transforms from the weights with the `transforms()` method (we need these so we can convert our images into the same format as the pretrained EffNetB2 was trained on).\n","3. Create a pretrained model instance by passing the weights to an instance of [`torchvision.models.efficientnet_b2`](https://pytorch.org/vision/stable/models/generated/torchvision.models.efficientnet_b2.html#efficientnet-b2).\n","4. Freeze the base layers in the model.\n","5. Update the classifier head to suit our own data."],"metadata":{"id":"xw7rx_JturLs"}},{"cell_type":"code","source":["# 1. Setup pretrained EffNetB2 weights\n","effnetb2_weights = torchvision.models.EfficientNet_B2_Weights.DEFAULT\n","\n","# 2. Get EffNetB2 transforms\n","effnetb2_transforms = effnetb2_weights.transforms()\n","\n","# 3. Setup pretrained model\n","effnetb2 = torchvision.models.efficientnet_b2(weights=effnetb2_weights) # could also use weights=\"DEFAULT\"\n","\n","# 4. Freeze the base layers in the model (this will freeze all layers to begin with)\n","for param in effnetb2.parameters():\n","    param.requires_grad = False"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NOTV4O1HswnU","executionInfo":{"status":"ok","timestamp":1689327247440,"user_tz":-540,"elapsed":3713,"user":{"displayName":"Hee Seong Yang","userId":"11765788859671095501"}},"outputId":"2d4c6a7d-1b0d-4875-8a26-4d801d04dfd8"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stderr","text":["Downloading: \"https://download.pytorch.org/models/efficientnet_b2_rwightman-bcdf34b7.pth\" to /root/.cache/torch/hub/checkpoints/efficientnet_b2_rwightman-bcdf34b7.pth\n","100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 35.2M/35.2M [00:02<00:00, 14.1MB/s]\n"]}]},{"cell_type":"markdown","source":["Now to change the classifier head, let's first inspect it using the `classifier` attribute of our model."],"metadata":{"id":"4kXfJltcwDCT"}},{"cell_type":"code","source":["# Check out EffNetB2 classifier head\n","effnetb2.classifier"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8kIeuPOWwAyZ","executionInfo":{"status":"ok","timestamp":1689327266098,"user_tz":-540,"elapsed":4,"user":{"displayName":"Hee Seong Yang","userId":"11765788859671095501"}},"outputId":"d9e4ba42-9344-4ae3-bb15-8f20b98e09ce"},"execution_count":8,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Sequential(\n","  (0): Dropout(p=0.3, inplace=True)\n","  (1): Linear(in_features=1408, out_features=1000, bias=True)\n",")"]},"metadata":{},"execution_count":8}]},{"cell_type":"markdown","source":["Excellent! To change the classifier head to suit our problem, let's replace the `out_features` variable with the same number of classes we have (in our case, `out_features=3`, one for pizza, steak, sushi).\n","\n","> **Note**: This process of changing the output layers/classifier head will be dependent on the problem you're working on. For example, if you wanted a different *number* of outputs or a different *kind* of output, you would have to change the output layers accordingly."],"metadata":{"id":"Y64tHkJXwJ-j"}},{"cell_type":"code","source":["# 5. Update the classifier head\n","effnetb2.classifier = nn.Sequential(\n","    nn.Dropout(p=0.3, inplace=True),    # keep dropout layer same\n","    nn.Linear(in_features=1408,         # keep in_features same\n","              out_features=3))          # change out_features to suit our number of classes"],"metadata":{"id":"dRfdMMxswGQP","executionInfo":{"status":"ok","timestamp":1689327678221,"user_tz":-540,"elapsed":491,"user":{"displayName":"Hee Seong Yang","userId":"11765788859671095501"}}},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":["### 3.1 Creating a function to make an EffNetB2 feature extractor\n","\n","Looks like our EffNetB2 feature extractor is ready to go, however, since there are quite a few steps involved here, how about we turn the code above into a function we can reuse later?\n","\n","We'll call it `create_effnetb2_model()` and it'll take a customizable number of classes and a random seed parameter for reproducibility.\n","\n","Ideally, it will return an EffNetB2 feature extractor along with its assosciated transforms."],"metadata":{"id":"KQGwl3h1xsla"}},{"cell_type":"code","source":[],"metadata":{"id":"4Hd0Rq4wxq2p"},"execution_count":null,"outputs":[]}]}