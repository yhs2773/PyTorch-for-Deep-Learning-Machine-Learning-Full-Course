{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"toc_visible":true,"authorship_tag":"ABX9TyOO/Zm4Bz0oJ2lvtyA2e5n7"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# 5. PyTorch Going Modular\n","\n","This section answers the question, \"how do I turn my notebook code into Python scripts?\"\n","## What is going modular?\n","\n","Turning notebook code into python scripts that offer similar functionality.\n","\n","For instance:\n","- `data_setup.py`: file to prepare and downloaded data if needed\n","- `engine.py`: file with training functions\n","- `model_builder.py` or `model.py`: file to create PyTorch model\n","- `train.py`: file to load all needed files to train a PyTorch model\n","- `utils.py`: file with helpful functions\n","\n","> **Note:** Naming of the files can be different depending on the cases\n","\n","## Why would you want to go modular?\n","\n","**Production code** is code that runs to offer a service to someone or something\n","\n","It's debatable, but generally one may use Python scripts to provide service, whereas Netflix uses notebooks\n","\n","### Pros and cons of notebooks vs Python scripts\n","\n","Notebooks\n","- Pros\n","    - Easy to experiment/get started\n","    - Easy to share\n","    - Very visual\n","- Cons\n","    - Versioning can be hard\n","    - Hard to use only specific parts\n","    - Text and graphics can interfere with the code\n","\n","Python scripts (files ending in `.py`)\n","- Pros\n","    - Can package code together (saves rewritting similar code across differnet notebooks)\n","    - Can use git for versioning\n","    - Many open source projects use sciprts\n","    - Larger projects can be run on cloud vendors (not as much support for notebooks)\n","- Cons\n","    - Experimenting isn't as visual (usually have to run the whole script rather than one cell)\n","\n","### My workflow\n","\n","I think it's best, to begin with a notebook and then move to scripts if needed\n","\n","### PyTorch in the wild\n","\n","There are instructions on how to run the code via scripts in code repositories\n","\n","Below is an example CLI code for training a model and a picture of definitions from https://www.learnpytorch.io/05_pytorch_going_modular/\n","\n","- `python train.py --model MODEL_NAME --batch_size BATCH_SIZE --lr LEARNING_RATE --num_epochs NUM_EPOCHS`\n","\n","![image](https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/05-python-train-command-line-annotated.png)\n","\n","## What we're going to cover\n","\n","Main goal: **turn useful notebook code cells into reusable Python files**\n","\n","This save us from writing code repeatedly\n","\n","There are 2 notebooks for this section:\n","1. [05. Going Modular: Part 1 (cell mode)](https://github.com/mrdbourke/pytorch-deep-learning/blob/main/going_modular/05_pytorch_going_modular_cell_mode.ipynb) - jupyternotebook style file\n","2. [05. Going Modular: Part 2 (script mode)](https://github.com/mrdbourke/pytorch-deep-learning/blob/main/going_modular/05_pytorch_going_modular_script_mode.ipynb) - Python scripts style file\n","\n","### Why two parts?\n","\n","To see side-by-side what's happening and they differ\n","\n","### What we're working towards\n","\n","2 main takeaways:\n","1. Ability to train the model using command line: `python train.py`\n","2. Directory structure of reusable Python scripts\n","\n","### Things to note\n","\n","- **Docstrings**: For reproducibility and understandability, we'll be creating docstrings in each of the functions/classes\n","\n","- **Imports at the top of scripts**: All scripts require their input modules imported at the start of the script\n","    - Import all required libraries at the beginning"],"metadata":{"id":"7O-fEtDVunny"}},{"cell_type":"markdown","source":["## 0. Cell mode vs. script mode"],"metadata":{"id":"biqceRfC1D21"}},{"cell_type":"markdown","source":["## 1. Get data\n","\n","Data retrieval process is same as in notebook 04\n","\n","Use `requests` module to download `.zip`file and unzip it"],"metadata":{"id":"iLsjOot36AfF"}},{"cell_type":"code","source":["import os\n","import requests\n","import zipfile\n","from pathlib import Path\n","\n","# Setup path to data folder\n","data_path = Path(\"data/\")\n","image_path = data_path / \"pizza_steak_sushi\"\n","\n","# If the image folder doesn't exist, download it and prepare it\n","if image_path.is_dir():\n","    (f\"{image_path} directory exists\")\n","else:\n","    print(f\"Did not find {image_path} directory, creating one\")\n","    image_path.mkdir(parents=True, exist_ok=True)\n","\n","# Download data\n","with open(data_path / \"pizza_steak_sushi.zip\", \"wb\") as f:\n","    request = requests.get(\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi.zip\")\n","    print(\"Downloading data\")\n","    f.write(request.content)\n","\n","# Unzip data\n","with zipfile.ZipFile(data_path / \"pizza_steak_sushi.zip\", \"r\") as zip_ref:\n","    print(\"Unzipping data\")\n","    zip_ref.extractall(image_path)\n","\n","# Remove zip file\n","os.remove(data_path / \"pizza_steak_sushi.zip\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"F7PBDFiK0QsG","executionInfo":{"status":"ok","timestamp":1684893646849,"user_tz":-540,"elapsed":1019,"user":{"displayName":"Hee Seong Yang","userId":"11765788859671095501"}},"outputId":"d0ab8ebc-3963-488e-c624-cd564593b125"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading data\n","Unzipping data\n"]}]},{"cell_type":"markdown","source":["## 2. Create Datasets and DataLoaders (`data_setup.py`)\n","\n","`Dataset` and `DataLoader` code will be created into the `create_dataloaders()` function\n","\n","We write it to file using the line `%%writefile going_modular/data_setup.py`\n","\n","> Ensure to create a directory first prior to using `%%writefile` magic command"],"metadata":{"id":"jtFlH_vQ7URf"}},{"cell_type":"code","source":["# Create the directory if it doesn't exist\n","os.makedirs(\"going_modular\", exist_ok=True)"],"metadata":{"id":"3EovB20B_9r9"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sAVfykInuhXB","executionInfo":{"status":"ok","timestamp":1684893646850,"user_tz":-540,"elapsed":18,"user":{"displayName":"Hee Seong Yang","userId":"11765788859671095501"}},"outputId":"64a383f2-0a43-499b-e2fd-0138af2c0f8f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Overwriting going_modular/data_setup.py\n"]}],"source":["%%writefile going_modular/data_setup.py\n","\"\"\"\n","Contains functionality for creating PyTorch DataLoaders for\n","image classification data.\n","\"\"\"\n","# loading libraries\n","import os\n","\n","from torchvision import datasets, transforms\n","from torch.utils.data import DataLoader\n","\n","NUM_WORKERS = os.cpu_count()\n","\n","def create_dataloaders(\n","    train_dir: str,\n","    test_dir: str,\n","    transform: transforms.Compose,\n","    batch_size: int,\n","    num_workers: int=NUM_WORKERS\n","):\n","    \"\"\"\n","    Creates training and testing DataLoaders.\n","\n","    Takes in a training directory and testing directory path and turns\n","    them into PyTorch Datasets and then into PyTorch DataLoaders.\n","\n","    Args:\n","        train_dir: Path to training directory\n","        test_dir: Path to testing directory\n","        transform: torchvision transforms to perform on training and testing data\n","        batch_size: Number of samples per batch in each of the DataLoaders\n","        num_workers: An integer for number of workers per DataLoader\n","\n","    Returns:\n","        A tuple of (train_dataloader, test_dataloader, class_names)\n","        Where class_names is a list of the target classes\n","        Example usage:\n","            train_dataloader, test_dataloader, class_names = \\\n","                = create_dataloaders(\n","                    train_dir=path/to/train_dir,\n","                    test_dir=path/to/test_dir,\n","                    trasnform=some_transform,\n","                    batch_size=32,\n","                    num_workers=4)\n","    \"\"\"\n","    # Use ImageFolder to create dataset(s)\n","    train_data = datasets.ImageFolder(root=train_dir, transform=transform)\n","    test_data = datasets.ImageFolder(root=test_dir, transform=transform)\n","\n","    # Get class class_names\n","    class_names = train_data.classes\n","\n","    # Turn images into dataloaders\n","    train_dataloader = DataLoader(\n","        dataset=train_data,\n","        batch_size=batch_size,\n","        shuffle=True,\n","        num_workers=num_workers,\n","        pin_memory=True\n","    )\n","    \n","    test_dataloader = DataLoader(\n","        dataset=test_data,\n","        batch_size=batch_size,\n","        shuffle=False,\n","        num_workers=num_workers,\n","        pin_memory=True\n","    )\n","\n","    return train_dataloader, test_dataloader, class_names"]},{"cell_type":"markdown","source":["We can now use the function within `data_setup.py`\n","```\n","# Import data_setup.py\n","from going_modular import data_setup\n","\n","# Create train/test dataloader and get class names as a list\n","train_dataloader, test_dataloader, class_names = data_setup.create_dataloaders()\n","```"],"metadata":{"id":"zXjmAzNE_DdN"}},{"cell_type":"markdown","source":["## 3. Making a model (`model_builder.py`)\n","\n","Now it's time to put the model into a script to reuse it.\n","\n","Let's put `TinyVGG()` model class into a script with the line `%%writefile going_modular/model_builder.py`"],"metadata":{"id":"gL38X5b9Aj9M"}},{"cell_type":"code","source":["%%writefile going_modular/model_builder.py\n","\"\"\"\n","Contains PyTorch model code to instantiate a TinyVGG model\n","\"\"\"\n","import torch\n","from torch import nn\n","\n","class TinyVGG(nn.Module):\n","    \"\"\"\n","    Creates the TinyVGG architecture\n","\n","    Replicates the TinyVGG architecture from the CNN explainer website in PyTorch\n","    See the original architecture here: https://poloclub.github.io/cnn-explainer/\n","\n","    Args:\n","        input_shape: An integer indicating number of input channels\n","        hidden_units: An integer indicating number of hidden units between layers\n","        output_shape: An integer indicating number of output units\n","    \"\"\"\n","    def __init__(self, input_shape: int, hidden_units: int, output_shape: int) -> None:\n","        super().__init__()\n","        self.conv_block_1 = nn.Sequential(\n","            nn.Conv2d(in_channels=input_shape,\n","                      out_channels=hidden_units,\n","                      kernel_size=3,\n","                      stride=1,\n","                      padding=0),\n","            nn.ReLU(),\n","            nn.Conv2d(in_channels=hidden_units,\n","                      out_channels=hidden_units,\n","                      kernel_size=3,\n","                      stride=1,\n","                      padding=0),\n","            nn.ReLU(),\n","            nn.MaxPool2d(kernel_size=2,\n","                         stride=2)\n","        )\n","        self.conv_block_2 = nn.Sequential(\n","            nn.Conv2d(in_channels=hidden_units,\n","                      out_channels=hidden_units,\n","                      kernel_size=3,\n","                      stride=1,\n","                      padding=0),\n","            nn.ReLU(),\n","            nn.Conv2d(in_channels=hidden_units,\n","                      out_channels=hidden_units,\n","                      kernel_size=3,\n","                      stride=1,\n","                      padding=0),\n","            nn.ReLU(),\n","            nn.MaxPool2d(kernel_size=2,\n","                         stride=2)\n","        )\n","        self.classifier = nn.Sequential(\n","            nn.Flatten(),\n","            # Where did this in_features shape come from?\n","            # It's because each layer of our network compresses and changes the shape of our input data\n","            nn.Linear(in_features=hidden_units*13*13,\n","                      out_features=output_shape)\n","        )\n","    \n","    def forward(self, x):\n","        x = self.conv_block_1(x)\n","        x = self.conv_block_2(x)\n","        x = self.classifier(x)\n","        return x\n","        # return self.classifier(self.conv_block_2(self.conv_block_1(x))) # <- leverage the benefits of operator fusion"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ym4jL6DY-dzw","executionInfo":{"status":"ok","timestamp":1684893646850,"user_tz":-540,"elapsed":16,"user":{"displayName":"Hee Seong Yang","userId":"11765788859671095501"}},"outputId":"1e13763a-6777-419f-8c93-952841a8170f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Overwriting going_modular/model_builder.py\n"]}]},{"cell_type":"markdown","source":["Now we can import the model using\n","```\n","import torch\n","# Import model_builder.py\n","from going_modular import model_builder\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","\n","# Instantiate an instance of the model from the \"model_builder.py\" script\n","# torch.manual_seed(42)\n","model = model_builder.TinyVGG(input_shape=3,\n","                              hidden_units=10,\n","                              output_shape=len(class_names)).to(device)\n","```"],"metadata":{"id":"B9TfeWTwD6si"}},{"cell_type":"markdown","source":["## 4. Creating `train_step()` and `test_step()` functions and `train()` to combine them\n","\n","We wrote several training functions in notebook 04:\n","1. `train_step()` - takes in a model, a DataLoader, a loss function and an optimizer and trains the model on the DataLoader.\n","2. `test_step()` - takes in a model, a DataLoader and a loss function and evaluates the model on the DataLoader.\n","3. `train()` - performs 1. and 2. together for a given number of epochs and returns a results dictionary.\n","\n","We can put above functions to `engine.py` with the line `%%writefile going_modular/engine.py`"],"metadata":{"id":"tENIw0XGEjhm"}},{"cell_type":"code","source":["%%writefile going_modular/engine.py\n","\"\"\"\n","Contains functions for training and testing a PyTorch model\n","\"\"\"\n","import torch\n","\n","from tqdm.auto import tqdm\n","from typing import Dict, List, Tuple\n","\n","def train_step(model: torch.nn.Module,\n","               dataloader: torch.utils.data.DataLoader,\n","               loss_fn: torch.nn.Module,\n","               optimizer: torch.optim.Optimizer,\n","               device: torch.device) -> Tuple[float, float]:\n","    \"\"\"\n","    Trains a PyTorch model for a single epoch\n","\n","    Turns a target PyTorch model to training mode and then\n","    runs through all of the required training steps (forward\n","    pass, loss calculation, optimizer step)\n","\n","    Args:\n","        model: A PyTorch model to be trained\n","        dataloader: A DataLoader instance for the model to be trained on\n","        loss_fn: A PyTorch loss function to minimize\n","        optimizer: A PyTorch optimizer to help minimize the loss function\n","        device: A target device to compute on (e.g. \"cuda\" or \"cpu\")\n","\n","    Returns:\n","        A tuple of training loss and training accuracy metrics\n","        in the form (train_loss, train_acc)\n","\n","        For example: (0.1112, 0.8743)\n","    \"\"\"\n","    # Put model in train mode\n","    model.train()\n","\n","    # Setup train loss and train accuracy values\n","    train_loss, train_acc = 0, 0\n","\n","    # Loop through data loader data batches\n","    for batch, (X, y) in enumerate(dataloader):\n","        # Send data to the target device\n","        X, y = X.to(device), y.to(device)\n","        # 1. Forward pass\n","        y_pred_logits = model(X)\n","        y_pred_probs = torch.softmax(y_pred_logits, dim=1)\n","        y_pred = torch.argmax(y_pred_probs, dim=1)\n","\n","        # 2. Calculate and accumulate loss\n","        loss = loss_fn(y_pred_logits, y)\n","        train_loss += loss.item()\n","\n","        # 3. Optimizer zero grad\n","        optimizer.zero_grad()\n","\n","        # 4. Loss backward\n","        loss.backward()\n","\n","        # 5. Optimizer step\n","        optimizer.step()\n","\n","        # Calculate and accumulate accuracy metric across all batches\n","        train_acc += (y_pred==y).sum().item()/len(y_pred_logits)\n","\n","    # Adjust the metrics to get average loss and accuracy\n","    train_loss /= len(dataloader)\n","    train_acc /= len(dataloader)\n","\n","    return train_loss, train_acc\n","\n","def test_step(model: torch.nn.Module,\n","              dataloader: torch.utils.data.DataLoader,\n","              loss_fn: torch.nn.Module,\n","              device: torch.device) -> Tuple[float, float]:\n","    \"\"\"\n","    Tests a PyTorch model for a single epoch\n","\n","    Turns a target PyTorch model to \"eval\" mode and then\n","    performs a forward pass on a testing dataset\n","\n","    Args:\n","        model: A PyTorch model to be tested\n","        dataloader: A DataLoader instance for the model to be tested on\n","        loss_fn: A PyTorch loss function to calculate loss on the test data\n","        device: A target device to compute on (e.g. \"cuda\" or \"cpu\")\n","\n","    Returns:\n","        A tuple of testing loss and testing accuracy metrics\n","        in the form (test_loss, test_accuracy)\n","\n","        For example: (0.0223, 0.8985)\n","    \"\"\"\n","    # Put the model in eval mode\n","    model.eval()\n","\n","    # Setup test loss and test accuracy values\n","    test_loss, test_acc = 0, 0\n","\n","    # Turn on the inference mode context manager\n","    with torch.inference_mode():\n","        # Loop through DataLoader batches\n","        for batch, (X, y) in enumerate(dataloader):\n","            # Send data to the target device\n","            X, y = X.to(device), y.to(device)\n","\n","            # 1. Forward pass\n","            test_pred_logits = model(X)\n","            test_pred_probs = torch.softmax(test_pred_logits, dim=1)\n","            test_pred = torch.argmax(test_pred_probs, dim=1)\n","\n","            # 2. Calculate and accumulate loss\n","            loss = loss_fn(test_pred_logits, y)\n","            test_loss += loss.item()\n","\n","            # Calculate and accumulate accuracy\n","            test_acc += (test_pred==y).sum().item()/len(test_pred_logits)\n","        \n","    # Adjust metrics to get average loss and accuracy\n","    test_loss /= len(dataloader)\n","    test_acc /= len(dataloader)\n","\n","    return test_loss, test_acc\n","\n","def train(model: torch.nn.Module,\n","          train_dataloader: torch.utils.data.DataLoader,\n","          test_dataloader: torch.utils.data.DataLoader,\n","          loss_fn: torch.nn.Module,\n","          optimizer: torch.optim.Optimizer,\n","          epochs: int,\n","          device: torch.device) -> Dict[str, List]:\n","    \"\"\"\n","    Trains and tests a PyTorch model\n","\n","    Passes a target PyTorch models through train_step() and test_step()\n","    functions for a number of epochs, training and testing the model\n","    in the same epoch loop\n","\n","    Calculates, prints and stores evaluation metrics throughout\n","\n","    Args:\n","        model: A PyTorch model to be trained and tested\n","        train_dataloader: A DataLoader instance for the model to be trained on\n","        test_dataloader: A DataLoader instance for the model to be tested on\n","        loss_fn: A PyTorch loss function to calculate loss on both datasets\n","        optimizer: A PyTorch optimizer to help minimize the loss function\n","        epochs: An integer indicating how many epochs to train for\n","        device: A target device to compute on (e.g. \"cuda\" or \"cpu\")\n","\n","    Returns:\n","        A dictionary of training and testing loss as well as accuracy metrics\n","        Each metric has a value in a list for each epoch\n","\n","        In the form: {train_loss: [...],\n","                      train_acc: [...],\n","                      test_loss: [...],\n","                      test_acc: [...]}\n","        For example if training for epochs=2:\n","                     {train_loss: [2.0616, 1.0537],\n","                      train_acc: [0.3945, 0.3945],\n","                      test_loss: [1.2641, 1.5706],\n","                      test_acc: [0.3400, 0.2973]}\n","    \"\"\"\n","    # Create an empty results dictionary\n","    results = {\"train_loss\": [],\n","               \"train_acc\": [],\n","               \"test_loss\": [],\n","               \"test_acc\": []}\n","\n","    # Loop through training and testing steps for a number of epochs\n","    for epoch in tqdm(range(epochs)):\n","        train_loss, train_acc = train_step(model=model,\n","                                           dataloader=train_dataloader,\n","                                           loss_fn=loss_fn,\n","                                           optimizer=optimizer,\n","                                           device=device)\n","        test_loss, test_acc = test_step(model=model,\n","                                        dataloader=test_dataloader,\n","                                        loss_fn=loss_fn,\n","                                        device=device)\n","        \n","        # Print out what's happening\n","        print(f\"Epoch: {epoch} | Train loss: {train_loss:.4f} | Train acc: {train_acc:.4f} | Test loss: {test_loss:.4f} | Test acc: {test_acc:.4f}\")\n","\n","        # Update results dictionary\n","        results[\"train_loss\"].append(train_loss)\n","        results[\"train_acc\"].append(train_acc)\n","        results[\"test_loss\"].append(test_loss)\n","        results[\"test_acc\"].append(test_acc)\n","\n","    # Return the filled results at the end of the epochs\n","    return results"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ChBq0DCyD5e8","executionInfo":{"status":"ok","timestamp":1684893646851,"user_tz":-540,"elapsed":15,"user":{"displayName":"Hee Seong Yang","userId":"11765788859671095501"}},"outputId":"9afcf229-8d87-4cf2-99ef-1228bda14a87"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Overwriting going_modular/engine.py\n"]}]},{"cell_type":"markdown","source":["Now we've got the `engine.py` script, we can import it via:\n","```\n","# Import engine.py\n","from going_modular import engine\n","\n","# Use train() by calling it from engine.py\n","engine.train(...)\n","```"],"metadata":{"id":"ZHEJC1A7Ob3n"}},{"cell_type":"markdown","source":["## 5. Creating a function to save the model (`utils.py`)\n","\n","It's a common practice to store helper functions in a file called `utils.py` (short for utilities)\n","\n","Let's save our `save_model()` function to a file called `utils.py` with the line `%%writefile going_modular/utils.py`"],"metadata":{"id":"xfqrBcZLOq5b"}},{"cell_type":"code","source":["%%writefile going_modular/utils.py\n","\"\"\"\n","Contains various utility functions for PyTorch model training and saving\n","\"\"\"\n","import torch\n","from pathlib import Path\n","\n","def save_model(model: torch.nn.Module,\n","               target_dir: str,\n","               model_name: str):\n","    \"\"\"\n","    Saves a PyTorch model to a target directory\n","\n","    Args:\n","        model: A target PyTorch model to save\n","        target_dir: A directory for saving the model to\n","        model_name: A filename for the saved model. Should include\n","            either \".pth\" or \".pt\" as the file extension\n","    Example usage:\n","        save_model(model=model_0,\n","                   target_dir=\"models\",\n","                   model_name=\"05_going_modular_tinyvgg_model.pth\")\n","    \"\"\"\n","    # Create target directory\n","    target_dir_path = Path(target_dir)\n","    target_dir_path.mkdir(parents=True,\n","                          exist_ok=True)\n","    \n","    # Create model save path\n","    assert model_name.endswith(\".pth\") or model_name.endswith(\".pt\")\n","    model_save_path = target_dir_path / model_name\n","\n","    # Save the model state_dict()\n","    print(f\"[INFO] Saving model to: {model_save_path}\")\n","    torch.save(obj=model.state_dict(),\n","               f=model_save_path)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sI0VRhAEGydY","executionInfo":{"status":"ok","timestamp":1684893646851,"user_tz":-540,"elapsed":13,"user":{"displayName":"Hee Seong Yang","userId":"11765788859671095501"}},"outputId":"27e4344a-9dd5-4a85-c720-198df6edd160"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Overwriting going_modular/utils.py\n"]}]},{"cell_type":"markdown","source":["Now we can save the model via:\n","```\n","# Import utils.py\n","from going_modular import utils\n","\n","# Save a model to file\n","save_model(model=model...\n","           target_dir=...\n","           model_name=...)\n","```"],"metadata":{"id":"blfF00xyaNnC"}},{"cell_type":"markdown","source":["## 6. Train, evaluate, and save the model (`train.py`)\n","\n","Usually you'll find repositories that combine all files put together in `train.py` file\n","\n","In our `train.py` file, we'll combine all scripts, so we can train a PyTorch model using single line of code:\n","```\n","python train.py\n","```\n","To create `train.py` we'll go through the following steps:\n","1. Import libraries and all scripts from `going_modular` directory like `data_setup`, `engine`, `model_builder`, and `utils`\n","2. **Note**: `train.py` will also be inside going_modular dir, so we can import scripts via `import ...` rather than `from going_modular import ...`\n","3. Setup hyperparameters like batch size, learning rate, epochs, and hidden units (these could be set in the future via [Python's argpase](https://docs.python.org/3/library/argparse.html))\n","4. Setup train and test directories\n","5. Setup device-agnostic code\n","6. Create necessary data transforms\n","7. Create DataLoader using `data_setup.py`\n","8. Create a model using `model_builder.py`\n","9. Setup loss function and optimizer\n","10. Train the model using `engine.py`\n","11. Save the model using `utils.py`\n","\n","We can create script using the line: `%%writefile going_modular/train.py`"],"metadata":{"id":"ZqptmORhab0x"}},{"cell_type":"code","source":["%%writefile going_modular/train.py\n","\"\"\"\n","Trains a PyTorch image classification model using device-agnostic code\n","\"\"\"\n","\n","import os\n","import torch\n","import data_setup, engine, model_builder, utils\n","\n","from torchvision import transforms\n","\n","# Setup hyperparameters\n","NUM_EPOCHS = 5\n","BATCH_SIZE = 32\n","HIDDEN_UNITS = 10\n","LEARNING_RATE = 0.001\n","\n","# Setup directories\n","train_dir = \"data/pizza_steak_sushi/train\"\n","test_dir = \"data/pizza_steak_sushi/test\"\n","\n","# Setup device-agnostic code\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","\n","# Create transforms\n","data_transform = transforms.Compose([\n","    transforms.Resize(size=(64, 64)),\n","    transforms.ToTensor()\n","])\n","\n","# Create DataLoader using data_setup.py\n","train_dataloader, test_dataloader, class_names = data_setup.create_dataloaders(\n","    train_dir=train_dir,\n","    test_dir=test_dir,\n","    transform=data_transform,\n","    batch_size=BATCH_SIZE\n",")\n","\n","# Create model using model_builder.py\n","model = model_builder.TinyVGG(\n","    input_shape=3,\n","    hidden_units=HIDDEN_UNITS,\n","    output_shape=len(class_names)\n",").to(device)\n","\n","# Set loss function and optimizer\n","loss_fn = torch.nn.CrossEntropyLoss()\n","optimizer = torch.optim.Adam(params=model.parameters(),\n","                             lr=LEARNING_RATE)\n","\n","# Start training using engine.py\n","engine.train(model=model,\n","             train_dataloader=train_dataloader,\n","             test_dataloader=test_dataloader,\n","             loss_fn=loss_fn,\n","             optimizer=optimizer,\n","             epochs=NUM_EPOCHS,\n","             device=device)\n","\n","# Save the model using utils.py\n","utils.save_model(model=model,\n","                 target_dir=\"models\",\n","                 model_name=\"05_going_modular_script_mode_tinyvgg_model.pth\")"],"metadata":{"id":"P8CnWz01JIdH","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1684893646852,"user_tz":-540,"elapsed":12,"user":{"displayName":"Hee Seong Yang","userId":"11765788859671095501"}},"outputId":"c7a8a235-7d1c-408f-8fa5-c0a038709e14"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Overwriting going_modular/train.py\n"]}]},{"cell_type":"markdown","source":["Now we can train a model by running the following line:\n","```\n","python train.py\n","```\n","\n","And if we wanted to, we could adjust our train.py file to use argument flag inputs with Python's argparse module, this would allow us to provide different hyperparameter settings like previously discussed:\n","```\n","python train.py --model MODEL_NAME --batch_size BATCH_SIZE --lr LEARNING_RATE --num_epochs NUM_EPOCHS\n","```"],"metadata":{"id":"cG4bnmg3cmVC"}},{"cell_type":"code","source":["!python going_modular/train.py"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Xt9t6CO7ch-O","executionInfo":{"status":"ok","timestamp":1684893657269,"user_tz":-540,"elapsed":10427,"user":{"displayName":"Hee Seong Yang","userId":"11765788859671095501"}},"outputId":"10e3e33a-7a1d-406e-c9fb-04fd63cf75c8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["  0% 0/5 [00:00<?, ?it/s]Epoch: 0 | Train loss: 1.1023 | Train acc: 0.3633 | Test loss: 1.0853 | Test acc: 0.5417\n"," 20% 1/5 [00:01<00:06,  1.53s/it]Epoch: 1 | Train loss: 1.0974 | Train acc: 0.2812 | Test loss: 1.0676 | Test acc: 0.5417\n"," 40% 2/5 [00:03<00:04,  1.51s/it]Epoch: 2 | Train loss: 1.0792 | Train acc: 0.4336 | Test loss: 1.0743 | Test acc: 0.2812\n"," 60% 3/5 [00:05<00:03,  1.93s/it]Epoch: 3 | Train loss: 1.0415 | Train acc: 0.5352 | Test loss: 1.1004 | Test acc: 0.2812\n"," 80% 4/5 [00:06<00:01,  1.75s/it]Epoch: 4 | Train loss: 1.0518 | Train acc: 0.3828 | Test loss: 1.0684 | Test acc: 0.3532\n","100% 5/5 [00:08<00:00,  1.68s/it]\n","[INFO] Saving model to: models/05_going_modular_script_mode_tinyvgg_model.pth\n"]}]}]}